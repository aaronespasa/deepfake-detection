{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataset of faces dividing them in fake and real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries, constants and pre-trained MTCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install FaceNet-PyTorch if it's not already installed (it includes the pre-trained MTCNN model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facenet-pytorch in /Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages (2.5.2)\n",
      "Requirement already satisfied: Pillow in /Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages (8.4.0)\n",
      "Requirement already satisfied: pandas in /Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages (1.3.5)\n",
      "Requirement already satisfied: requests in /Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages (from facenet-pytorch) (2.26.0)\n",
      "Requirement already satisfied: numpy in /Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages (from facenet-pytorch) (1.22.0)\n",
      "Requirement already satisfied: torchvision in /Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages (from facenet-pytorch) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages (from requests->facenet-pytorch) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages (from requests->facenet-pytorch) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages (from requests->facenet-pytorch) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages (from requests->facenet-pytorch) (3.1)\n",
      "Requirement already satisfied: torch in /Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages (from torchvision->facenet-pytorch) (1.11.0a0+gitbfe5ad2)\n",
      "Requirement already satisfied: typing_extensions in /Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages (from torch->torchvision->facenet-pytorch) (4.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install facenet-pytorch Pillow pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import cv2\n",
    "from random import randint\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Running on device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the pre-trained MTCNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Guide on how to use MTCNN](https://www.kaggle.com/code/timesler/guide-to-mtcnn-in-facenet-pytorch/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import ORIGINAL_VIDEOS_FOLDER, FAKE_VIDEOS_FOLDER, FACES_FOLDER, FACES_REAL, FACES_FAKE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists of fake and real videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_VIDEOS = os.listdir(ORIGINAL_VIDEOS_FOLDER)\n",
    "FAKE_VIDEOS = os.listdir(FAKE_VIDEOS_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "post_process avoids image normalization to be able to plot the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MTCNN in evaluation mode\n",
    "# -> Evaluation deactivates Dropout layers, which speeds up the inference\n",
    "mtcnn = MTCNN(select_largest=False, post_process=False, device=device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(num_samples=3):\n",
    "    \"\"\"Plot the first frame of fake video and the extracted face\"\"\"\n",
    "    _, ax = plt.subplots(nrows=num_samples, ncols=2, figsize=(18, 10))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        video_idx = randint(0, len(FAKE_VIDEOS)-1)\n",
    "\n",
    "        # Fake video\n",
    "        fake_video = cv2.VideoCapture(\n",
    "            os.path.join(FAKE_VIDEOS_FOLDER, FAKE_VIDEOS[video_idx]\n",
    "        ))\n",
    "        # Get the first frame\n",
    "        _, fake_frame = fake_video.read()\n",
    "        # BGR -> RGB\n",
    "        fake_frame = cv2.cvtColor(fake_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Convert the image (numpy array) to a Pillow Image\n",
    "        frame = Image.fromarray(fake_frame)\n",
    "        face = mtcnn(frame)\n",
    "\n",
    "        # After being processed by MTCNN, the face has the shape: (CHANNELS, HEIGHT, WIDTH)\n",
    "        # we have to change the change so that it has the shape (HEIGHT, WIDTH, CHANNELS)\n",
    "        face = face.permute(1, 2, 0)\n",
    "\n",
    "        # Remember that the output of the model is a tensor where the pixel values are float values\n",
    "        # To be able to plot the face, we have to convert it into a numpy array of integer values\n",
    "        #\n",
    "        # If the image is in the GPU, we also have to move it to the CPU\n",
    "        face = face.cpu().detach().int().numpy()\n",
    "\n",
    "        # Plot the frames\n",
    "        ax[i, 0].imshow(fake_frame)\n",
    "        ax[i, 1].imshow(face)\n",
    "        ax[i, 0].set_title(\"Fake (full image)\")\n",
    "        ax[i, 1].set_title(\"Fake (face)\")\n",
    "        ax[i, 0].axis(\"off\")\n",
    "        ax[i, 1].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.__version__ and your torchvision version with torchvision.__version__ and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb Cell 19'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb#ch0000018?line=0'>1</a>\u001b[0m plot_example()\n",
      "\u001b[1;32m/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb Cell 18'\u001b[0m in \u001b[0;36mplot_example\u001b[0;34m(num_samples)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb#ch0000017?line=16'>17</a>\u001b[0m \u001b[39m# Convert the image (numpy array) to a Pillow Image\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb#ch0000017?line=17'>18</a>\u001b[0m frame \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(fake_frame)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb#ch0000017?line=18'>19</a>\u001b[0m face \u001b[39m=\u001b[39m mtcnn(frame)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb#ch0000017?line=20'>21</a>\u001b[0m \u001b[39m# After being processed by MTCNN, the face has the shape: (CHANNELS, HEIGHT, WIDTH)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb#ch0000017?line=21'>22</a>\u001b[0m \u001b[39m# we have to change the change so that it has the shape (HEIGHT, WIDTH, CHANNELS)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb#ch0000017?line=22'>23</a>\u001b[0m face \u001b[39m=\u001b[39m face\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py:258\u001b[0m, in \u001b[0;36mMTCNN.forward\u001b[0;34m(self, img, save_path, return_prob)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=226'>227</a>\u001b[0m \u001b[39m\"\"\"Run MTCNN face detection on a PIL image or numpy array. This method performs both\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=227'>228</a>\u001b[0m \u001b[39mdetection and extraction of faces, returning tensors representing detected faces rather\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=228'>229</a>\u001b[0m \u001b[39mthan the bounding boxes. To access bounding boxes, see the MTCNN.detect() method below.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=253'>254</a>\u001b[0m \u001b[39m>>> face_tensor, prob = mtcnn(img, save_path='face.png', return_prob=True)\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=254'>255</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=256'>257</a>\u001b[0m \u001b[39m# Detect faces\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=257'>258</a>\u001b[0m batch_boxes, batch_probs, batch_points \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdetect(img, landmarks\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=258'>259</a>\u001b[0m \u001b[39m# Select faces\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=259'>260</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_all:\n",
      "File \u001b[0;32m~/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py:313\u001b[0m, in \u001b[0;36mMTCNN.detect\u001b[0;34m(self, img, landmarks)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=272'>273</a>\u001b[0m \u001b[39m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=273'>274</a>\u001b[0m \n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=274'>275</a>\u001b[0m \u001b[39mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=308'>309</a>\u001b[0m \u001b[39m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=309'>310</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=311'>312</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=312'>313</a>\u001b[0m     batch_boxes, batch_points \u001b[39m=\u001b[39m detect_face(\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=313'>314</a>\u001b[0m         img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_face_size,\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=314'>315</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monet,\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=315'>316</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mthresholds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfactor,\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=316'>317</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=317'>318</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=319'>320</a>\u001b[0m boxes, probs, points \u001b[39m=\u001b[39m [], [], []\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=320'>321</a>\u001b[0m \u001b[39mfor\u001b[39;00m box, point \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(batch_boxes, batch_points):\n",
      "File \u001b[0;32m~/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py:79\u001b[0m, in \u001b[0;36mdetect_face\u001b[0;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py?line=75'>76</a>\u001b[0m boxes\u001b[39m.\u001b[39mappend(boxes_scale)\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py?line=76'>77</a>\u001b[0m image_inds\u001b[39m.\u001b[39mappend(image_inds_scale)\n\u001b[0;32m---> <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py?line=78'>79</a>\u001b[0m pick \u001b[39m=\u001b[39m batched_nms(boxes_scale[:, :\u001b[39m4\u001b[39;49m], boxes_scale[:, \u001b[39m4\u001b[39;49m], image_inds_scale, \u001b[39m0.5\u001b[39;49m)\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py?line=79'>80</a>\u001b[0m scale_picks\u001b[39m.\u001b[39mappend(pick \u001b[39m+\u001b[39m offset)\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py?line=80'>81</a>\u001b[0m offset \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m boxes_scale\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py:68\u001b[0m, in \u001b[0;36mbatched_nms\u001b[0;34m(boxes, scores, idxs, iou_threshold)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=65'>66</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _batched_nms_vanilla(boxes, scores, idxs, iou_threshold)\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=66'>67</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=67'>68</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _batched_nms_coordinate_trick(boxes, scores, idxs, iou_threshold)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/jit/_trace.py:1118\u001b[0m, in \u001b[0;36m_script_if_tracing.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/jit/_trace.py?line=1113'>1114</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/jit/_trace.py?line=1114'>1115</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/jit/_trace.py?line=1115'>1116</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tracing():\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/jit/_trace.py?line=1116'>1117</a>\u001b[0m         \u001b[39m# Not tracing, don't do anything\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/jit/_trace.py?line=1117'>1118</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/jit/_trace.py?line=1119'>1120</a>\u001b[0m     compiled_fn \u001b[39m=\u001b[39m script(wrapper\u001b[39m.\u001b[39m__original_fn)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/jit/_trace.py?line=1120'>1121</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m compiled_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py:87\u001b[0m, in \u001b[0;36m_batched_nms_coordinate_trick\u001b[0;34m(boxes, scores, idxs, iou_threshold)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=84'>85</a>\u001b[0m offsets \u001b[39m=\u001b[39m idxs\u001b[39m.\u001b[39mto(boxes) \u001b[39m*\u001b[39m (max_coordinate \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(boxes))\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=85'>86</a>\u001b[0m boxes_for_nms \u001b[39m=\u001b[39m boxes \u001b[39m+\u001b[39m offsets[:, \u001b[39mNone\u001b[39;00m]\n\u001b[0;32m---> <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=86'>87</a>\u001b[0m keep \u001b[39m=\u001b[39m nms(boxes_for_nms, scores, iou_threshold)\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=87'>88</a>\u001b[0m \u001b[39mreturn\u001b[39;00m keep\n",
      "File \u001b[0;32m~/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py:34\u001b[0m, in \u001b[0;36mnms\u001b[0;34m(boxes, scores, iou_threshold)\u001b[0m\n\u001b[1;32m      <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnms\u001b[39m(boxes: Tensor, scores: Tensor, iou_threshold: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=9'>10</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=10'>11</a>\u001b[0m \u001b[39m    Performs non-maximum suppression (NMS) on the boxes according\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=11'>12</a>\u001b[0m \u001b[39m    to their intersection-over-union (IoU).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=31'>32</a>\u001b[0m \u001b[39m        by NMS, sorted in decreasing order of scores\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=32'>33</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=33'>34</a>\u001b[0m     _assert_has_ops()\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=34'>35</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mtorchvision\u001b[39m.\u001b[39mnms(boxes, scores, iou_threshold)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py:62\u001b[0m, in \u001b[0;36m_assert_has_ops\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=59'>60</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assert_has_ops\u001b[39m():\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=60'>61</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_ops():\n\u001b[0;32m---> <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=61'>62</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=62'>63</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load custom C++ ops. This can happen if your PyTorch and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=63'>64</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtorchvision versions are incompatible, or if you had errors while compiling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=64'>65</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtorchvision from source. For further information on the compatible versions, check \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=65'>66</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/pytorch/vision#installation for the compatibility matrix. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=66'>67</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPlease check your PyTorch version with torch.__version__ and your torchvision \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=67'>68</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mversion with torchvision.__version__ and verify if they are compatible, and if not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=68'>69</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mplease reinstall torchvision so that it matches your PyTorch install.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=69'>70</a>\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.__version__ and your torchvision version with torchvision.__version__ and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1296x720 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new folder where we'll store the face images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ../data/faces/real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf ../data/faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(FACES_FOLDER)\n",
    "os.mkdir(FACES_REAL)\n",
    "# os.mkdir(FACES_FAKE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faces.csv \u001b[1m\u001b[36mfake\u001b[m\u001b[m      \u001b[1m\u001b[36mreal\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to store the face images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_name_from_video_name(video_name):\n",
    "    \"\"\"Get the face name from the video name\"\"\"\n",
    "    img_name = video_name.split(\".mp4\")[0] # Remove the .mp4 extension\n",
    "    return img_name + \"_face.png\" # Add the _face.png extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frame(fake_frame, video_name, faces_folder):\n",
    "    fake_frame = cv2.cvtColor(fake_frame, cv2.COLOR_BGR2RGB) # BGR -> RGB\n",
    "\n",
    "    frame = Image.fromarray(fake_frame)\n",
    "    face = mtcnn(frame) # extract the face from the frame\n",
    "\n",
    "    # if the face is not found on the image, we do not save it\n",
    "    if face is None: return\n",
    "\n",
    "    # (CHANNELS, HEIGHT, WIDTH) -> (HEIGHT, WIDTH, CHANNELS)\n",
    "    face = face.permute(1, 2, 0)\n",
    "    # face is a tensor where the pixel values are float32 values\n",
    "    # -> move it to the CPU, convert it to int values and then to a numpy array \n",
    "    face = face.cpu().detach().numpy().astype(np.uint8)\n",
    "\n",
    "    # Save the face\n",
    "    img_name = get_face_name_from_video_name(video_name)\n",
    "    Image.fromarray(face).save(os.path.join(faces_folder, img_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_face(videos_folder, videos_names, faces_folder, is_real=None):\n",
    "    \"\"\"Save the face of the video in the faces_folder\"\"\"\n",
    "    for i in range(len(videos_names)):\n",
    "        fake_video = cv2.VideoCapture(\n",
    "            os.path.join(videos_folder, videos_names[i]\n",
    "        ))\n",
    "\n",
    "        # get a frame every 30 frames of fake_video only if is_real is True\n",
    "        # otherwise, get a frame every frame of fake_video\n",
    "        while fake_video.isOpened():\n",
    "            if is_real is True:\n",
    "                count = 0\n",
    "                ret, real_frame = fake_video.read()\n",
    "                if ret is True:\n",
    "                    # the name has to be different for each frame\n",
    "                    save_frame(real_frame, f\"{videos_names[i]}_f{count}\", faces_folder)\n",
    "\n",
    "                    count += 30 # i.e. at 30 fps, this advances one second\n",
    "                    real_frame.set(cv2.CAP_PROP_POS_FRAMES, count)\n",
    "                else:\n",
    "                    real_frame.release()\n",
    "                    break\n",
    "            else:\n",
    "                ret, fake_frame = fake_video.read() # Get the first frame\n",
    "                if ret is True:\n",
    "                    save_frame(fake_frame, videos_names[i], faces_folder)\n",
    "                else:\n",
    "                    fake_frame.release()\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.__version__ and your torchvision version with torchvision.__version__ and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb Cell 30'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb#ch0000029?line=0'>1</a>\u001b[0m \u001b[39m# Store the faces of the real videos:\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb#ch0000029?line=1'>2</a>\u001b[0m save_face(ORIGINAL_VIDEOS_FOLDER, ORIGINAL_VIDEOS, FACES_REAL, is_real\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb Cell 29'\u001b[0m in \u001b[0;36msave_face\u001b[0;34m(videos_folder, videos_names, faces_folder, is_real)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb#ch0000028?line=12'>13</a>\u001b[0m ret, real_frame \u001b[39m=\u001b[39m fake_video\u001b[39m.\u001b[39mread()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb#ch0000028?line=13'>14</a>\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb#ch0000028?line=14'>15</a>\u001b[0m     \u001b[39m# the name has to be different for each frame\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb#ch0000028?line=15'>16</a>\u001b[0m     save_frame(real_frame, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mvideos_names[i]\u001b[39m}\u001b[39;49;00m\u001b[39m_f\u001b[39;49m\u001b[39m{\u001b[39;49;00mcount\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, faces_folder)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb#ch0000028?line=17'>18</a>\u001b[0m     count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m30\u001b[39m \u001b[39m# i.e. at 30 fps, this advances one second\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb#ch0000028?line=18'>19</a>\u001b[0m     real_frame\u001b[39m.\u001b[39mset(cv2\u001b[39m.\u001b[39mCAP_PROP_POS_FRAMES, count)\n",
      "\u001b[1;32m/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb Cell 28'\u001b[0m in \u001b[0;36msave_frame\u001b[0;34m(fake_frame, video_name, faces_folder)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb#ch0000027?line=1'>2</a>\u001b[0m fake_frame \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(fake_frame, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB) \u001b[39m# BGR -> RGB\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb#ch0000027?line=3'>4</a>\u001b[0m frame \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(fake_frame)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb#ch0000027?line=4'>5</a>\u001b[0m face \u001b[39m=\u001b[39m mtcnn(frame) \u001b[39m# extract the face from the frame\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb#ch0000027?line=6'>7</a>\u001b[0m \u001b[39m# if the face is not found on the image, we do not save it\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron/Documents/github/deepfake-detection/dataset/create_face_dataset.ipynb#ch0000027?line=7'>8</a>\u001b[0m \u001b[39mif\u001b[39;00m face \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py:258\u001b[0m, in \u001b[0;36mMTCNN.forward\u001b[0;34m(self, img, save_path, return_prob)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=226'>227</a>\u001b[0m \u001b[39m\"\"\"Run MTCNN face detection on a PIL image or numpy array. This method performs both\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=227'>228</a>\u001b[0m \u001b[39mdetection and extraction of faces, returning tensors representing detected faces rather\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=228'>229</a>\u001b[0m \u001b[39mthan the bounding boxes. To access bounding boxes, see the MTCNN.detect() method below.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=253'>254</a>\u001b[0m \u001b[39m>>> face_tensor, prob = mtcnn(img, save_path='face.png', return_prob=True)\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=254'>255</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=256'>257</a>\u001b[0m \u001b[39m# Detect faces\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=257'>258</a>\u001b[0m batch_boxes, batch_probs, batch_points \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdetect(img, landmarks\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=258'>259</a>\u001b[0m \u001b[39m# Select faces\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=259'>260</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_all:\n",
      "File \u001b[0;32m~/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py:313\u001b[0m, in \u001b[0;36mMTCNN.detect\u001b[0;34m(self, img, landmarks)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=272'>273</a>\u001b[0m \u001b[39m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=273'>274</a>\u001b[0m \n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=274'>275</a>\u001b[0m \u001b[39mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=308'>309</a>\u001b[0m \u001b[39m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=309'>310</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=311'>312</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=312'>313</a>\u001b[0m     batch_boxes, batch_points \u001b[39m=\u001b[39m detect_face(\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=313'>314</a>\u001b[0m         img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_face_size,\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=314'>315</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monet,\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=315'>316</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mthresholds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfactor,\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=316'>317</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=317'>318</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=319'>320</a>\u001b[0m boxes, probs, points \u001b[39m=\u001b[39m [], [], []\n\u001b[1;32m    <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py?line=320'>321</a>\u001b[0m \u001b[39mfor\u001b[39;00m box, point \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(batch_boxes, batch_points):\n",
      "File \u001b[0;32m~/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py:79\u001b[0m, in \u001b[0;36mdetect_face\u001b[0;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py?line=75'>76</a>\u001b[0m boxes\u001b[39m.\u001b[39mappend(boxes_scale)\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py?line=76'>77</a>\u001b[0m image_inds\u001b[39m.\u001b[39mappend(image_inds_scale)\n\u001b[0;32m---> <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py?line=78'>79</a>\u001b[0m pick \u001b[39m=\u001b[39m batched_nms(boxes_scale[:, :\u001b[39m4\u001b[39;49m], boxes_scale[:, \u001b[39m4\u001b[39;49m], image_inds_scale, \u001b[39m0.5\u001b[39;49m)\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py?line=79'>80</a>\u001b[0m scale_picks\u001b[39m.\u001b[39mappend(pick \u001b[39m+\u001b[39m offset)\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py?line=80'>81</a>\u001b[0m offset \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m boxes_scale\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py:68\u001b[0m, in \u001b[0;36mbatched_nms\u001b[0;34m(boxes, scores, idxs, iou_threshold)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=65'>66</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _batched_nms_vanilla(boxes, scores, idxs, iou_threshold)\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=66'>67</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=67'>68</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _batched_nms_coordinate_trick(boxes, scores, idxs, iou_threshold)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/jit/_trace.py:1118\u001b[0m, in \u001b[0;36m_script_if_tracing.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/jit/_trace.py?line=1113'>1114</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/jit/_trace.py?line=1114'>1115</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/jit/_trace.py?line=1115'>1116</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tracing():\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/jit/_trace.py?line=1116'>1117</a>\u001b[0m         \u001b[39m# Not tracing, don't do anything\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/jit/_trace.py?line=1117'>1118</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/jit/_trace.py?line=1119'>1120</a>\u001b[0m     compiled_fn \u001b[39m=\u001b[39m script(wrapper\u001b[39m.\u001b[39m__original_fn)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torch/jit/_trace.py?line=1120'>1121</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m compiled_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py:87\u001b[0m, in \u001b[0;36m_batched_nms_coordinate_trick\u001b[0;34m(boxes, scores, idxs, iou_threshold)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=84'>85</a>\u001b[0m offsets \u001b[39m=\u001b[39m idxs\u001b[39m.\u001b[39mto(boxes) \u001b[39m*\u001b[39m (max_coordinate \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(boxes))\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=85'>86</a>\u001b[0m boxes_for_nms \u001b[39m=\u001b[39m boxes \u001b[39m+\u001b[39m offsets[:, \u001b[39mNone\u001b[39;00m]\n\u001b[0;32m---> <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=86'>87</a>\u001b[0m keep \u001b[39m=\u001b[39m nms(boxes_for_nms, scores, iou_threshold)\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=87'>88</a>\u001b[0m \u001b[39mreturn\u001b[39;00m keep\n",
      "File \u001b[0;32m~/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py:34\u001b[0m, in \u001b[0;36mnms\u001b[0;34m(boxes, scores, iou_threshold)\u001b[0m\n\u001b[1;32m      <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnms\u001b[39m(boxes: Tensor, scores: Tensor, iou_threshold: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=9'>10</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=10'>11</a>\u001b[0m \u001b[39m    Performs non-maximum suppression (NMS) on the boxes according\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=11'>12</a>\u001b[0m \u001b[39m    to their intersection-over-union (IoU).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=31'>32</a>\u001b[0m \u001b[39m        by NMS, sorted in decreasing order of scores\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=32'>33</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=33'>34</a>\u001b[0m     _assert_has_ops()\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/ops/boxes.py?line=34'>35</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mtorchvision\u001b[39m.\u001b[39mnms(boxes, scores, iou_threshold)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py:62\u001b[0m, in \u001b[0;36m_assert_has_ops\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=59'>60</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assert_has_ops\u001b[39m():\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=60'>61</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_ops():\n\u001b[0;32m---> <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=61'>62</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=62'>63</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load custom C++ ops. This can happen if your PyTorch and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=63'>64</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtorchvision versions are incompatible, or if you had errors while compiling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=64'>65</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtorchvision from source. For further information on the compatible versions, check \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=65'>66</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/pytorch/vision#installation for the compatibility matrix. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=66'>67</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPlease check your PyTorch version with torch.__version__ and your torchvision \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=67'>68</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mversion with torchvision.__version__ and verify if they are compatible, and if not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=68'>69</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mplease reinstall torchvision so that it matches your PyTorch install.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/aaron/miniforge3/envs/torch4arm/lib/python3.9/site-packages/torchvision/extension.py?line=69'>70</a>\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.__version__ and your torchvision version with torchvision.__version__ and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install."
     ]
    }
   ],
   "source": [
    "# Store the faces of the real videos:\n",
    "save_face(ORIGINAL_VIDEOS_FOLDER, ORIGINAL_VIDEOS, FACES_REAL, is_real=True)\n",
    "# Store the faces of the fake videos:\n",
    "# save_face(FAKE_VIDEOS_FOLDER, FAKE_VIDEOS, FACES_FAKE, is_real=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the images names with their label as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_faces = os.listdir(FACES_REAL)\n",
    "real_faces = np.array(list(zip(real_faces, [\"real\"]*len(real_faces))))\n",
    "fake_faces = os.listdir(FACES_FAKE)\n",
    "fake_faces = np.array(list(zip(fake_faces, [\"fake\"]*len(fake_faces))))\n",
    "\n",
    "faces = np.concatenate((real_faces, fake_faces))\n",
    "\n",
    "# shuffle the data\n",
    "np.random.shuffle(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(faces, columns=[\"name\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12_06__talking_against_wall__0VR4Y891_face.png</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23_19__walking_outside_cafe_disgusted__WHQ1229...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03_18__walking_and_outside_surprised__22UBC0BS...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23_24__kitchen_pan__YR5OVD4S_face.png</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>06_25__outside_talking_still_laughing__MI9BDQ7...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13_02__secret_conversation__PLNVLO74_face.png</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24_19__podium_speech_happy__OF8LF68K_face.png</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27_02__hugging_happy__UGO2181S_face.png</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20_14__podium_speech_happy__B014BKVO_face.png</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12_07__walking_and_outside_surprised__0VN0A2T3...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>07_03__talking_angry_couch__WPT3Z2KN_face.png</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>04_21__talking_against_wall__NRI4PHWP_face.png</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>03_07__hugging_happy__BKLOCI1M_face.png</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21_02__outside_talking_still_laughing__Z0XHPQA...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14_20__exit_phone_room__B014BKVO_face.png</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20_14__hugging_happy__B014BKVO_face.png</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>03_14__walking_down_street_outside_angry__H0VQ...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>01_27__outside_talking_pan_laughing__S2YCUY48_...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>03_07__walking_and_outside_surprised__IFSURI9X...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>07_03__kitchen_pan__PWXXULHR_face.png</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>07_03__podium_speech_happy__IFSURI9X_face.png</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>14_13__walking_outside_cafe_disgusted__KMQ3AW6...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21_07__walking_down_indoor_hall_disgust__MKU99...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>06_12__outside_talking_pan_laughing__3K21NFNM_...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>07_03__hugging_happy__IFSURI9X_face.png</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>27__podium_speech_happy_face.png</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15_02__outside_talking_pan_laughing__I8G2LWD1_...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20_26__podium_speech_happy__1UEDQ3GY_face.png</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13_25__hugging_happy__7YMTAF29_face.png</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>03_15__kitchen_pan__5C1OMK8W_face.png</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name label\n",
       "0      12_06__talking_against_wall__0VR4Y891_face.png  fake\n",
       "1   23_19__walking_outside_cafe_disgusted__WHQ1229...  fake\n",
       "2   03_18__walking_and_outside_surprised__22UBC0BS...  fake\n",
       "3               23_24__kitchen_pan__YR5OVD4S_face.png  fake\n",
       "4   06_25__outside_talking_still_laughing__MI9BDQ7...  fake\n",
       "5       13_02__secret_conversation__PLNVLO74_face.png  fake\n",
       "6       24_19__podium_speech_happy__OF8LF68K_face.png  fake\n",
       "7             27_02__hugging_happy__UGO2181S_face.png  fake\n",
       "8       20_14__podium_speech_happy__B014BKVO_face.png  fake\n",
       "9   12_07__walking_and_outside_surprised__0VN0A2T3...  fake\n",
       "10      07_03__talking_angry_couch__WPT3Z2KN_face.png  fake\n",
       "11     04_21__talking_against_wall__NRI4PHWP_face.png  fake\n",
       "12            03_07__hugging_happy__BKLOCI1M_face.png  fake\n",
       "13  21_02__outside_talking_still_laughing__Z0XHPQA...  fake\n",
       "14          14_20__exit_phone_room__B014BKVO_face.png  fake\n",
       "15            20_14__hugging_happy__B014BKVO_face.png  fake\n",
       "16  03_14__walking_down_street_outside_angry__H0VQ...  fake\n",
       "17  01_27__outside_talking_pan_laughing__S2YCUY48_...  fake\n",
       "18  03_07__walking_and_outside_surprised__IFSURI9X...  fake\n",
       "19              07_03__kitchen_pan__PWXXULHR_face.png  fake\n",
       "20      07_03__podium_speech_happy__IFSURI9X_face.png  fake\n",
       "21  14_13__walking_outside_cafe_disgusted__KMQ3AW6...  fake\n",
       "22  21_07__walking_down_indoor_hall_disgust__MKU99...  fake\n",
       "23  06_12__outside_talking_pan_laughing__3K21NFNM_...  fake\n",
       "24            07_03__hugging_happy__IFSURI9X_face.png  fake\n",
       "25                   27__podium_speech_happy_face.png  real\n",
       "26  15_02__outside_talking_pan_laughing__I8G2LWD1_...  fake\n",
       "27      20_26__podium_speech_happy__1UEDQ3GY_face.png  fake\n",
       "28            13_25__hugging_happy__7YMTAF29_face.png  fake\n",
       "29              03_15__kitchen_pan__5C1OMK8W_face.png  fake"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(FACES_FOLDER, \"faces.csv\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70876cdcfe169b991766b3b6fc1c26dc5a770678ea7f06db3355d725f5c5506d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch4arm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
